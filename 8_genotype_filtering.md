Or go back to summary statistics [here](https://github.com/evansbenj/BIO722.md/blob/main/7_summary_statistics.md).

# Genotype filtering

Even after all the steps we've done (indel realignment, deduping), there undoubtedly still are crappy genotype calls.  These arise from low or patchy coverage in some individuals, duplicated regions in the sample or reference, sequencing errors, mapping errors, and more. `GATK` has a procedure called base quality score recalibration (`BQSR`) which allows you to alter quality scores to more accurately reflect the error rate, which can vary in sample-specific ways depending on genomic context and technical variation. We're not going to go over this today, but please feel free to ask me about this later if you want.

What we will do, and what is widely done before analysis, is filter our vcf files to get rid of low quality genotypes.  There are many programs that can do this, including [vcftools](http://vcftools.sourceforge.net/man_latest.html), [bcftools](http://samtools.github.io/bcftools/bcftools.html), and [GATK](https://gatk.broadinstitute.org/hc/en-us).  Probably the most sophisticated approach is to use GATK's variant quality score recalibration (`VQSR`) which uses a model-based approach to assess quality scores and filter genotypes based on multi-dimensional criteria. This generally requires a large (>30 individuals) sample side and some fairly strong assumptions (e.g. a set of known variants). Instead, today, we will implement a hard-cutoff filter, also using GATK.
